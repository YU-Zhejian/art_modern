# Design Topics

Here lists various topics related to the design of `art_modern`.

## Motivation

ART is an excellent software for simulating reads from a reference genome. However, it comes with the following limitations:

- The implementation is not parallelized.
- The implementation may consume too much memory or fail on genomes that contain a limited number of chromosomes of enormous (larger than `signed int32`) sizes.
- The implementation may consume too much memory or fail on transcriptome or template collection that contains an enormous number of contigs (cDNAs or other types of template molecules).
- The implementation only supports unified coverage, which is not suitable for transcriptome simulation where each cDNA molecule has its own expression level.
- The implementation outputs alignments in SAM format, which is not space-efficient. We also spot some SAM records generated by `art_illumina` whose query length inferred from CIGAR strings differs from the actual query length.

So, we developed `art_modern` with the following ideas:

- Parallelization is implemented using another layer of abstraction, simulation job, which can be taken as a unit of work in one thread, etc.
- Writer for SAM output format was re-implemented using [HTSLib](https://www.htslib.org/), which allows supporting BAM and headless SAM/BAM output format with minimal modifications of code.
- Multiple FASTA parsers were added. For example, the `htslib` parser allows on-disk random access of enormous genomes without reading them into memory, while the `stream` parser allows streaming of FASTA files. All coordinates are now int64-based.
- The low-level I/O routines are made asynchronized to improve performance.
- Support other random number generators like Intel OneAPI Math Kernel Library (OneMKL).

## Parallelism

The parallelization strategy of different modes and input parsers are as follows:

| Parser \ Mode | `wgs`     | `trans`   | `templ`   |
|---------------|-----------|-----------|-----------|
| `memory`      | Coverage  | Batch     | Batch     |
| `htslib`      | Coverage  | **ERROR** | **ERROR** |
| `stream`      | **ERROR** | Batch     | Batch     |

Here, "coverage" means that all contigs are passed to all threads with coverage divided by the number of threads, while "batch" means that the entire data were split into batches (partitioning the existing in-memory data structure for `memory` parser or `--i-batch_size` for `stream` parser) and passed to different threads. The current batch-based design may be suboptimal if transcript/template lengths or coverages are not evenly distributed. You're recommended to shuffle the input data to avoid such problems using, i.e., `seqkit shuffle`. For even larger data, a proposed MPI-based parallelization strategy is in [TODO.md](TODO.md).

## Random Generators

### Bits Generation

The current random number generation function in each library is [MT19937](https://doi.org/10.1145/272991.272995), which may not be the best choice for performance-critical applications. However, it is the most widely used, well-known, of moderate performance and cycle, and is implemented in all random number generator libraries (namely, Boost, GSL, STL, and Intel OneAPI MKL).

We choose not to use [Abseil Random](https://abseil.io/docs/cpp/guides/random) since (1) It is hard to bundle the entire Abseil random library with the project and (2) Its performance is not satisfying. Using Intel compiler, even absl::InsecureBitGen is slower than either boost::random::mt19937 or std::mt19937.

We choose not to use [cuRAND](https://docs.nvidia.com/cuda/curand/index.html) since it is hard to configure and its performance is not satisfying, which may be due to the time spent on copying data from GPU as the majority of our computation happens on CPU.

The current implementation also supports [PCG](https://www.pcg-random.org/) random generator experimentally. This random generator should be faster than STL random generators.

The current version does not support the specification of seed since it will result in reads with identical position and error profile in each thread. The current seed is set by the product of nanoseconds since epoch and hash of the current thread ID to allow each thread to generate different data.

### Distribution Sampling

The distribution sampling is currently implemented using each library's own implementation. For example; `boost::random::uniform_int_distribution` for Boost random generators, `std::uniform_int_distribution` for STL and PCG, `viRngUniform` for Intel MKL, and `gsl_rng_uniform_int` for GSL. Further decoupling may be needed.

## I/O

The generated read data, which is represented as `PairwiseAlignment` class (`src/lib/PairwiseAlignment.hh`), will be passed to an output dispatcher which dispatches the data to all output writers registered to it.

Output writers are asynchronous. The `PairwiseAlignment`s are firstly formatted by each thread (since setting of `bam1_t*` is time-consuming) and then passed to a multi-producer single-consumer lock-free queue, which is then consumed by the file appender in a separate thread.

We choose not to support [UCSC 2-bit](http://genome.ucsc.edu/FAQ/FAQformat.html#format7) format since Developing a 2-bit parser is time-consuming, especially when endianness (2-bit files allow both endianness), 64-bit offsets, masking, and other edge cases are considered. Additionally, whether the UCSC 2-bit format improves I/O performance is questionable. See [this HTSJDK PR](https://github.com/samtools/htsjdk/pull/1417) for details.

## Other Performance Bottlenecks

A majority of time was spent on generation of quality scores, which extensively calls `map::lower_bound()` function. This was addressed by using Walker's algorithm.
